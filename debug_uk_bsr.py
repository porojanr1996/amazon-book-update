#!/usr/bin/env python3
"""
Debug script pentru a vedea ce returneazÄƒ Playwright pentru UK
"""
import sys
from pathlib import Path
import asyncio

# Add project root to path
project_root = Path(__file__).parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from app.services.browser_pool import fetch_page
from app.utils.bsr_parser import parse_bsr
from bs4 import BeautifulSoup
import re

async def debug_uk_bsr(url: str):
    """Debug BSR extraction for UK URL"""
    print(f"\nğŸ” Debug pentru: {url}")
    print("=" * 60)
    
    # Clean URL
    clean_url = url.split('/ref')[0].split('?')[0].rstrip('/')
    if not clean_url.endswith('/'):
        clean_url += '/'
    
    print(f"ğŸ“¡ Fetching cu Playwright: {clean_url}")
    html = await fetch_page(clean_url, timeout=30000, retries=1)
    
    if not html:
        print("âŒ Nu s-a putut obÈ›ine HTML")
        return
    
    print(f"âœ… HTML obÈ›inut: {len(html)} caractere")
    
    # VerificÄƒ dacÄƒ e pagina de eroare
    if "Continue shopping" in html or "Conditions of Use" in html:
        print("âš ï¸  PaginÄƒ de eroare/blocare detectatÄƒ")
    
    # CautÄƒ BSR Ã®n HTML
    soup = BeautifulSoup(html, 'lxml')
    
    # CautÄƒ "Best Sellers Rank" sau "BSR"
    rank_text = soup.find_all(text=re.compile(r'Best.*Seller.*Rank|BSR', re.I))
    if rank_text:
        print(f"\nâœ… GÄƒsit text cu 'Best Sellers Rank':")
        for text in rank_text[:5]:  # Primele 5
            parent = text.parent if hasattr(text, 'parent') else None
            if parent:
                print(f"   {parent.get_text()[:200]}")
    else:
        print("\nâŒ Nu s-a gÄƒsit 'Best Sellers Rank' Ã®n HTML")
    
    # CautÄƒ div-ul SalesRank
    sales_rank_div = soup.find('div', {'id': 'SalesRank'})
    if sales_rank_div:
        print(f"\nâœ… GÄƒsit div#SalesRank:")
        print(f"   {sales_rank_div.get_text()[:500]}")
    else:
        print("\nâŒ Nu s-a gÄƒsit div#SalesRank")
    
    # CautÄƒ orice numÄƒr cu "#" urmat de "in"
    hash_pattern = re.compile(r'#(\d{1,3}(?:,\d{3})*)\s+in', re.I)
    matches = hash_pattern.findall(html)
    if matches:
        print(f"\nâœ… GÄƒsite pattern-uri '#X in':")
        for match in matches[:10]:  # Primele 10
            print(f"   #{match}")
    else:
        print("\nâŒ Nu s-au gÄƒsit pattern-uri '#X in'")
    
    # TesteazÄƒ parser-ul strict
    print(f"\nğŸ§ª Testare parser strict:")
    bsr = parse_bsr(html)
    if bsr:
        print(f"   âœ… BSR extras: #{bsr:,}")
    else:
        print(f"   âŒ Parser strict nu a gÄƒsit BSR")
    
    # SalveazÄƒ HTML pentru inspecÈ›ie manualÄƒ
    debug_file = f"/tmp/amazon_uk_debug_{url.split('/')[-1].split('?')[0]}.html"
    with open(debug_file, 'w', encoding='utf-8') as f:
        f.write(html)
    print(f"\nğŸ’¾ HTML salvat Ã®n: {debug_file}")

async def main():
    urls = [
        "https://www.amazon.co.uk/gp/product/B0DXN9WZGG?ref",
        "https://www.amazon.co.uk/gp/product/B08KPJVQN3?ref",
    ]
    
    for url in urls:
        await debug_uk_bsr(url)
        await asyncio.sleep(2)  # Delay Ã®ntre request-uri

if __name__ == '__main__':
    asyncio.run(main())

